{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classfier for automatic evaluation(sentiment\\category)\n",
    "# notice length condition doesn't need it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/admin8/anaconda3/envs/liangjiahui/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequences: 444101\n",
      "vocab size: 8904\n",
      "1734 247\n"
     ]
    }
   ],
   "source": [
    "#!-*- encoding=utf-8 -*-\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import *\n",
    "from keras.optimizers import *\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras import regularizers\n",
    "from keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, Callback\n",
    "import keras.backend as K\n",
    "from keras import metrics\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True  \n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "# hyper-parameters\n",
    "max_len = 15 \n",
    "max_vocab = 10000\n",
    "\n",
    "batch_size = 256\n",
    "\n",
    "emb_size = 100\n",
    "cnn_filter = 400\n",
    "cnn_kernel = 3\n",
    "\n",
    "pad_token = 0\n",
    "oov_token = 1\n",
    "start_token = 2\n",
    "end_token = 3\n",
    "\n",
    "# train and val dataset\n",
    "# choose the dataset by your need\n",
    "train_pos_path = '../dataset/yelp.train.1'\n",
    "train_neg_path = '../dataset/yelp.train.0'\n",
    "val_pos_path = '../dataset/yelp.dev.1'\n",
    "val_neg_path = '../dataset/yelp.dev.0'\n",
    "\n",
    "\n",
    "train = []\n",
    "val = []\n",
    "\n",
    "with open(train_pos_path, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        line = line.strip().lower().split(' ')\n",
    "        train.append([line, 1])\n",
    "        \n",
    "with open(train_neg_path, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        line = line.strip().lower().split(' ')\n",
    "        train.append([line, 0])\n",
    "        \n",
    "with open(val_pos_path, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        line = line.strip().lower().split(' ') \n",
    "        val.append([line, 1])\n",
    "        \n",
    "with open(val_neg_path, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        line = line.strip().lower().split(' ') \n",
    "        val.append([line, 0])\n",
    "\n",
    "print('sequences:', len(train))\n",
    "sys.stdout.flush()\n",
    "\n",
    "# load vocab file\n",
    "if os.path.exists('yelp-vocab.json'):\n",
    "    chars,id2char,char2id = json.load(open('yelp-vocab.json'))\n",
    "    id2char = {int(i):j for i,j in id2char.items()}\n",
    "    \n",
    "print('vocab size:', len(char2id))\n",
    "sys.stdout.flush()\n",
    "\n",
    "def str2id(s, start_end = False):\n",
    "    ids = [char2id.get(c, oov_token) for c in s]\n",
    "    if start_end:\n",
    "        ids = [start_token] + ids + [end_token]\n",
    "  \n",
    "    return ids\n",
    "\n",
    "def padding(x):\n",
    "    ml = max_len\n",
    "    x = [i + [0] * (ml-len(i)) for i in x]\n",
    "    x = np.array(x)\n",
    "    return x\n",
    "\n",
    "def train_generator(data):\n",
    "    x = []\n",
    "    y = []\n",
    "    \n",
    "    while True:\n",
    "        np.random.shuffle(data)    \n",
    "        for d in data:\n",
    "            text = d[0]\n",
    "            label = d[1]\n",
    "            text = str2id(text, start_end=False)\n",
    "            \n",
    "            x.append(text)\n",
    "            y.append(label)\n",
    "\n",
    "            if len(x) == batch_size:\n",
    "                x = padding(x)\n",
    "                yield [np.array(x),np.array(y)], None\n",
    "                x = []\n",
    "                y = []   \n",
    "\n",
    "def get_batch_num(data):\n",
    "    return len(data)//batch_size\n",
    "\n",
    "train_bs_num = get_batch_num(train)\n",
    "val_bs_num = get_batch_num(val)\n",
    "\n",
    "train_gen = train_generator(train)\n",
    "val_gen = train_generator(val)\n",
    "\n",
    "print(train_bs_num, val_bs_num)\n",
    "sys.stdout.flush()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 15)                0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 15, 100)           890400    \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 13, 400)           120400    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 401       \n",
      "=================================================================\n",
      "Total params: 1,011,201\n",
      "Trainable params: 1,011,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Model architecture of classifier\n",
    "x_in = Input(shape=(max_len, ))\n",
    "x = x_in\n",
    "x = Embedding(len(char2id), emb_size)(x)\n",
    "x = Conv1D(cnn_filter, cnn_kernel, padding='valid', activation='relu')(x)\n",
    "x = GlobalMaxPooling1D()(x)\n",
    "x_out = Dense(1, activation='sigmoid')(x)\n",
    "cls = Model(x_in ,x_out)\n",
    "cls.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 15)                0         \n",
      "_________________________________________________________________\n",
      "model_1 (Model)              (None, 1)                 1011201   \n",
      "=================================================================\n",
      "Total params: 1,011,201\n",
      "Trainable params: 1,011,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/admin8/anaconda3/envs/liangjiahui/lib/python3.6/site-packages/ipykernel_launcher.py:10: UserWarning: Output \"model_1\" missing from loss dictionary. We assume this was done on purpose, and we will not be expecting any data to be passed to \"model_1\" during training.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "# define a Model to train the classifier\n",
    "y_true = Input(shape=(1, ))\n",
    "x_in = Input(shape=(max_len, ))\n",
    "y_pred = cls(x_in)\n",
    "ce_loss = K.mean(K.binary_crossentropy(y_true, y_pred))\n",
    "cls_train = Model([x_in, y_true], y_pred)\n",
    "cls_train.add_loss(ce_loss)\n",
    "\n",
    "acc = metrics.binary_accuracy(y_true, y_pred)\n",
    "acc = K.sum(acc)/batch_size\n",
    "cls_train.compile(optimizer=Adam(5e-4, 0.5))\n",
    "\n",
    "cls_train.metrics_names.append('acc')\n",
    "cls_train.metrics_tensors.append(acc)\n",
    "cls_train.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 0, loss: [0.6959983, 0.3984375]\n",
      "val loss, [0.694147700964198, 0.4563986589068826]\n",
      "saving weights with best val: [0.694147700964198, 0.4563986589068826]\n",
      "iter: 50, loss: [0.6654447, 0.56640625]\n",
      "iter: 100, loss: [0.64249057, 0.57421875]\n",
      "iter: 150, loss: [0.5790658, 0.7265625]\n",
      "iter: 200, loss: [0.4871399, 0.82421875]\n",
      "iter: 250, loss: [0.40324253, 0.87109375]\n",
      "iter: 300, loss: [0.35704044, 0.86328125]\n",
      "iter: 350, loss: [0.24630098, 0.9296875]\n",
      "iter: 400, loss: [0.22908537, 0.921875]\n",
      "iter: 450, loss: [0.21124913, 0.90625]\n",
      "iter: 500, loss: [0.17588261, 0.953125]\n",
      "iter: 550, loss: [0.168956, 0.94140625]\n",
      "iter: 600, loss: [0.13130724, 0.95703125]\n",
      "iter: 650, loss: [0.12646672, 0.953125]\n",
      "iter: 700, loss: [0.11043687, 0.96875]\n",
      "iter: 750, loss: [0.16374522, 0.9453125]\n",
      "iter: 800, loss: [0.15667908, 0.94140625]\n",
      "iter: 850, loss: [0.10772397, 0.96875]\n",
      "iter: 900, loss: [0.15970168, 0.94140625]\n",
      "iter: 950, loss: [0.108915284, 0.96875]\n",
      "iter: 1000, loss: [0.09980579, 0.9765625]\n",
      "iter: 1050, loss: [0.09722699, 0.96875]\n",
      "iter: 1100, loss: [0.13827561, 0.953125]\n",
      "iter: 1150, loss: [0.09061752, 0.96484375]\n",
      "iter: 1200, loss: [0.10424997, 0.9609375]\n",
      "iter: 1250, loss: [0.15681368, 0.9453125]\n",
      "iter: 1300, loss: [0.09022941, 0.96875]\n",
      "iter: 1350, loss: [0.07761714, 0.96875]\n",
      "iter: 1400, loss: [0.10726562, 0.96484375]\n",
      "iter: 1450, loss: [0.13019742, 0.9453125]\n",
      "iter: 1500, loss: [0.06371527, 0.98046875]\n",
      "iter: 1550, loss: [0.16578865, 0.9375]\n",
      "iter: 1600, loss: [0.089887485, 0.9609375]\n",
      "iter: 1650, loss: [0.06408723, 0.98046875]\n",
      "iter: 1700, loss: [0.09726157, 0.96484375]\n",
      "val loss, [0.0941934040804141, 0.9660931174089069]\n",
      "saving weights with best val: [0.0941934040804141, 0.9660931174089069]\n",
      "iter: 1750, loss: [0.07580641, 0.98828125]\n",
      "iter: 1800, loss: [0.10728257, 0.96875]\n",
      "iter: 1850, loss: [0.0950674, 0.9609375]\n",
      "iter: 1900, loss: [0.046457935, 0.98828125]\n",
      "iter: 1950, loss: [0.09893224, 0.96875]\n",
      "iter: 2000, loss: [0.07013517, 0.98046875]\n",
      "iter: 2050, loss: [0.061059184, 0.984375]\n",
      "iter: 2100, loss: [0.06970076, 0.98046875]\n",
      "iter: 2150, loss: [0.06721324, 0.97265625]\n",
      "iter: 2200, loss: [0.077666946, 0.97265625]\n",
      "iter: 2250, loss: [0.062137038, 0.984375]\n",
      "iter: 2300, loss: [0.11298743, 0.95703125]\n",
      "iter: 2350, loss: [0.085176036, 0.96875]\n",
      "iter: 2400, loss: [0.04821262, 0.984375]\n",
      "iter: 2450, loss: [0.13123284, 0.96484375]\n",
      "iter: 2500, loss: [0.08493885, 0.96875]\n",
      "iter: 2550, loss: [0.07418458, 0.96875]\n",
      "iter: 2600, loss: [0.047068603, 0.9765625]\n",
      "iter: 2650, loss: [0.064901285, 0.9765625]\n",
      "iter: 2700, loss: [0.07265802, 0.9765625]\n",
      "iter: 2750, loss: [0.050103653, 0.98046875]\n",
      "iter: 2800, loss: [0.08998025, 0.97265625]\n",
      "iter: 2850, loss: [0.07518542, 0.97265625]\n",
      "iter: 2900, loss: [0.087175414, 0.97265625]\n",
      "iter: 2950, loss: [0.06612729, 0.9765625]\n",
      "iter: 3000, loss: [0.06302147, 0.97265625]\n",
      "iter: 3050, loss: [0.04657559, 0.98046875]\n",
      "iter: 3100, loss: [0.080140784, 0.98046875]\n",
      "iter: 3150, loss: [0.10158704, 0.97265625]\n",
      "iter: 3200, loss: [0.079534665, 0.95703125]\n",
      "iter: 3250, loss: [0.06406933, 0.97265625]\n",
      "iter: 3300, loss: [0.07766361, 0.9609375]\n",
      "iter: 3350, loss: [0.062138934, 0.984375]\n",
      "iter: 3400, loss: [0.073761806, 0.97265625]\n",
      "iter: 3450, loss: [0.0579674, 0.96484375]\n",
      "val loss, [0.07928499261950434, 0.9719762145748988]\n",
      "saving weights with best val: [0.07928499261950434, 0.9719762145748988]\n",
      "iter: 3500, loss: [0.06654179, 0.98046875]\n",
      "iter: 3550, loss: [0.08394795, 0.96484375]\n",
      "iter: 3600, loss: [0.043962844, 0.9921875]\n",
      "iter: 3650, loss: [0.06938096, 0.9765625]\n",
      "iter: 3700, loss: [0.06718004, 0.97265625]\n",
      "iter: 3750, loss: [0.08143501, 0.97265625]\n",
      "iter: 3800, loss: [0.08773604, 0.96484375]\n",
      "iter: 3850, loss: [0.07334164, 0.9765625]\n",
      "iter: 3900, loss: [0.06857656, 0.9921875]\n",
      "iter: 3950, loss: [0.0494258, 0.98828125]\n",
      "iter: 4000, loss: [0.05239223, 0.98046875]\n",
      "iter: 4050, loss: [0.05822386, 0.9765625]\n",
      "iter: 4100, loss: [0.102954745, 0.984375]\n",
      "iter: 4150, loss: [0.06296241, 0.97265625]\n",
      "iter: 4200, loss: [0.10957098, 0.97265625]\n",
      "iter: 4250, loss: [0.08655046, 0.96484375]\n",
      "iter: 4300, loss: [0.08288362, 0.9609375]\n",
      "iter: 4350, loss: [0.058576085, 0.96875]\n",
      "iter: 4400, loss: [0.08405838, 0.98046875]\n",
      "iter: 4450, loss: [0.052309576, 0.984375]\n",
      "iter: 4500, loss: [0.08820756, 0.98046875]\n",
      "iter: 4550, loss: [0.102264374, 0.9609375]\n",
      "iter: 4600, loss: [0.059025653, 0.984375]\n",
      "iter: 4650, loss: [0.08463031, 0.98046875]\n",
      "iter: 4700, loss: [0.06150565, 0.97265625]\n",
      "iter: 4750, loss: [0.06852627, 0.97265625]\n",
      "iter: 4800, loss: [0.043026607, 0.98828125]\n",
      "iter: 4850, loss: [0.088444486, 0.97265625]\n",
      "iter: 4900, loss: [0.034973085, 0.98828125]\n",
      "iter: 4950, loss: [0.06733857, 0.96875]\n",
      "iter: 5000, loss: [0.08826758, 0.96875]\n",
      "iter: 5050, loss: [0.12007381, 0.953125]\n",
      "iter: 5100, loss: [0.053912204, 0.98046875]\n",
      "iter: 5150, loss: [0.060894568, 0.97265625]\n",
      "iter: 5200, loss: [0.05628726, 0.97265625]\n",
      "val loss, [0.07365558619199977, 0.9742535425101214]\n",
      "saving weights with best val: [0.07365558619199977, 0.9742535425101214]\n",
      "iter: 5250, loss: [0.10143059, 0.9765625]\n",
      "iter: 5300, loss: [0.036368847, 0.9921875]\n",
      "iter: 5350, loss: [0.092227675, 0.98046875]\n",
      "iter: 5400, loss: [0.043227352, 0.984375]\n",
      "iter: 5450, loss: [0.059662018, 0.9765625]\n",
      "iter: 5500, loss: [0.06674743, 0.9765625]\n",
      "iter: 5550, loss: [0.03617765, 0.9921875]\n",
      "iter: 5600, loss: [0.0790522, 0.9765625]\n",
      "iter: 5650, loss: [0.03932613, 0.98828125]\n",
      "iter: 5700, loss: [0.03643954, 0.98828125]\n",
      "iter: 5750, loss: [0.06172934, 0.97265625]\n",
      "iter: 5800, loss: [0.08026058, 0.97265625]\n",
      "iter: 5850, loss: [0.055776693, 0.97265625]\n",
      "iter: 5900, loss: [0.047020502, 0.984375]\n",
      "iter: 5950, loss: [0.062783055, 0.98046875]\n",
      "iter: 6000, loss: [0.041015744, 0.984375]\n",
      "iter: 6050, loss: [0.060676757, 0.9921875]\n",
      "iter: 6100, loss: [0.054266818, 0.9921875]\n",
      "iter: 6150, loss: [0.07430964, 0.97265625]\n",
      "iter: 6200, loss: [0.060695507, 0.97265625]\n",
      "iter: 6250, loss: [0.04614237, 0.984375]\n",
      "iter: 6300, loss: [0.03954387, 0.984375]\n",
      "iter: 6350, loss: [0.026247816, 0.98828125]\n",
      "iter: 6400, loss: [0.048808146, 0.984375]\n",
      "iter: 6450, loss: [0.0440975, 0.97265625]\n",
      "iter: 6500, loss: [0.060573183, 0.9765625]\n",
      "iter: 6550, loss: [0.07967411, 0.96875]\n",
      "iter: 6600, loss: [0.060539547, 0.97265625]\n",
      "iter: 6650, loss: [0.07674832, 0.97265625]\n",
      "iter: 6700, loss: [0.0439986, 0.98828125]\n",
      "iter: 6750, loss: [0.05173005, 0.98046875]\n",
      "iter: 6800, loss: [0.09360397, 0.97265625]\n",
      "iter: 6850, loss: [0.04806476, 0.98046875]\n",
      "iter: 6900, loss: [0.07873019, 0.96875]\n",
      "val loss, [0.07202045062202432, 0.9750442813765182]\n",
      "saving weights with best val: [0.07202045062202432, 0.9750442813765182]\n",
      "iter: 6950, loss: [0.05756621, 0.98046875]\n",
      "iter: 7000, loss: [0.04231468, 0.984375]\n",
      "iter: 7050, loss: [0.0284341, 0.9921875]\n",
      "iter: 7100, loss: [0.05066921, 0.9765625]\n",
      "iter: 7150, loss: [0.064042054, 0.9765625]\n",
      "iter: 7200, loss: [0.0585082, 0.984375]\n",
      "iter: 7250, loss: [0.06848314, 0.98046875]\n",
      "iter: 7300, loss: [0.082380265, 0.9765625]\n",
      "iter: 7350, loss: [0.043123625, 0.98828125]\n",
      "iter: 7400, loss: [0.036835942, 0.98828125]\n",
      "iter: 7450, loss: [0.05401043, 0.98046875]\n",
      "iter: 7500, loss: [0.073199004, 0.984375]\n",
      "iter: 7550, loss: [0.064868554, 0.9765625]\n",
      "iter: 7600, loss: [0.027295412, 0.98828125]\n",
      "iter: 7650, loss: [0.07457004, 0.9765625]\n",
      "iter: 7700, loss: [0.07978787, 0.9765625]\n",
      "iter: 7750, loss: [0.039977837, 0.98828125]\n",
      "iter: 7800, loss: [0.078254074, 0.97265625]\n",
      "iter: 7850, loss: [0.088646255, 0.96484375]\n",
      "iter: 7900, loss: [0.060875833, 0.98046875]\n",
      "iter: 7950, loss: [0.07085304, 0.9765625]\n",
      "iter: 8000, loss: [0.065133356, 0.984375]\n",
      "iter: 8050, loss: [0.07032551, 0.97265625]\n",
      "iter: 8100, loss: [0.0903653, 0.95703125]\n",
      "iter: 8150, loss: [0.041850697, 0.984375]\n",
      "iter: 8200, loss: [0.0656979, 0.9765625]\n",
      "iter: 8250, loss: [0.03495074, 0.98828125]\n",
      "iter: 8300, loss: [0.04247407, 0.984375]\n",
      "iter: 8350, loss: [0.04434587, 0.98046875]\n",
      "iter: 8400, loss: [0.038848396, 0.9765625]\n",
      "iter: 8450, loss: [0.037569903, 0.99609375]\n",
      "iter: 8500, loss: [0.021041512, 0.99609375]\n",
      "iter: 8550, loss: [0.050067127, 0.9765625]\n",
      "iter: 8600, loss: [0.04231291, 0.984375]\n",
      "iter: 8650, loss: [0.062631585, 0.98046875]\n",
      "val loss, [0.06892444378873597, 0.9759931680161943]\n",
      "saving weights with best val: [0.06892444378873597, 0.9759931680161943]\n",
      "iter: 8700, loss: [0.025175024, 0.9921875]\n",
      "iter: 8750, loss: [0.017224543, 0.99609375]\n",
      "iter: 8800, loss: [0.03710637, 0.9921875]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 8850, loss: [0.0551171, 0.98046875]\n",
      "iter: 8900, loss: [0.017857458, 1.0]\n",
      "iter: 8950, loss: [0.05244284, 0.98046875]\n",
      "iter: 9000, loss: [0.061896898, 0.98828125]\n",
      "iter: 9050, loss: [0.029807704, 0.98828125]\n",
      "iter: 9100, loss: [0.03438906, 0.99609375]\n",
      "iter: 9150, loss: [0.042761177, 0.984375]\n",
      "iter: 9200, loss: [0.043924175, 0.98046875]\n",
      "iter: 9250, loss: [0.13694458, 0.953125]\n",
      "iter: 9300, loss: [0.037539743, 0.9921875]\n",
      "iter: 9350, loss: [0.062118966, 0.98046875]\n",
      "iter: 9400, loss: [0.030923488, 0.98828125]\n",
      "iter: 9450, loss: [0.047887776, 0.9765625]\n",
      "iter: 9500, loss: [0.058277555, 0.9765625]\n",
      "iter: 9550, loss: [0.04712882, 0.98828125]\n",
      "iter: 9600, loss: [0.083666965, 0.9765625]\n",
      "iter: 9650, loss: [0.09757673, 0.98046875]\n",
      "iter: 9700, loss: [0.06122382, 0.98828125]\n",
      "iter: 9750, loss: [0.086851284, 0.96875]\n",
      "iter: 9800, loss: [0.061638728, 0.98046875]\n",
      "iter: 9850, loss: [0.032365676, 0.98828125]\n",
      "iter: 9900, loss: [0.058945823, 0.984375]\n",
      "iter: 9950, loss: [0.034353685, 0.98828125]\n",
      "iter: 10000, loss: [0.036286082, 0.98828125]\n",
      "iter: 10050, loss: [0.054882694, 0.9921875]\n",
      "iter: 10100, loss: [0.03595879, 0.98828125]\n",
      "iter: 10150, loss: [0.027259927, 0.99609375]\n",
      "iter: 10200, loss: [0.058926284, 0.98828125]\n",
      "iter: 10250, loss: [0.04783963, 0.97265625]\n",
      "iter: 10300, loss: [0.046351597, 0.984375]\n",
      "iter: 10350, loss: [0.023352357, 0.9921875]\n",
      "iter: 10400, loss: [0.055045992, 0.98046875]\n",
      "val loss, [0.06784415195285068, 0.9764043522267206]\n",
      "saving weights with best val: [0.06784415195285068, 0.9764043522267206]\n",
      "iter: 10450, loss: [0.04759904, 0.98046875]\n",
      "iter: 10500, loss: [0.037872247, 0.98046875]\n",
      "iter: 10550, loss: [0.058545962, 0.984375]\n",
      "iter: 10600, loss: [0.027415972, 0.9921875]\n",
      "iter: 10650, loss: [0.06580499, 0.98046875]\n",
      "iter: 10700, loss: [0.02315921, 0.99609375]\n",
      "iter: 10750, loss: [0.051344667, 0.984375]\n",
      "iter: 10800, loss: [0.06469587, 0.984375]\n",
      "iter: 10850, loss: [0.037385833, 0.984375]\n",
      "iter: 10900, loss: [0.050663196, 0.984375]\n",
      "iter: 10950, loss: [0.036508396, 0.98828125]\n",
      "iter: 11000, loss: [0.028152155, 0.9921875]\n",
      "iter: 11050, loss: [0.058977004, 0.984375]\n",
      "iter: 11100, loss: [0.07128392, 0.98046875]\n",
      "iter: 11150, loss: [0.05252845, 0.98046875]\n",
      "iter: 11200, loss: [0.03620285, 0.98828125]\n",
      "iter: 11250, loss: [0.035314545, 0.98046875]\n",
      "iter: 11300, loss: [0.019075556, 0.99609375]\n",
      "iter: 11350, loss: [0.024272185, 0.9921875]\n",
      "iter: 11400, loss: [0.07710335, 0.98828125]\n",
      "iter: 11450, loss: [0.039227437, 0.984375]\n",
      "iter: 11500, loss: [0.021415498, 0.99609375]\n",
      "iter: 11550, loss: [0.036275327, 0.98828125]\n",
      "iter: 11600, loss: [0.042848967, 0.98828125]\n",
      "iter: 11650, loss: [0.029811697, 0.9921875]\n",
      "iter: 11700, loss: [0.053793143, 0.97265625]\n",
      "iter: 11750, loss: [0.028690463, 0.98828125]\n",
      "iter: 11800, loss: [0.031484038, 0.984375]\n",
      "iter: 11850, loss: [0.093764566, 0.96875]\n",
      "iter: 11900, loss: [0.035822418, 0.984375]\n",
      "iter: 11950, loss: [0.038493033, 0.984375]\n",
      "iter: 12000, loss: [0.030063843, 0.9921875]\n",
      "iter: 12050, loss: [0.036627218, 0.98828125]\n",
      "iter: 12100, loss: [0.10162561, 0.9609375]\n",
      "val loss, [0.07002063146094803, 0.9767364625506073]\n",
      "iter: 12150, loss: [0.031650167, 0.98828125]\n",
      "iter: 12200, loss: [0.05821999, 0.98046875]\n",
      "iter: 12250, loss: [0.028476115, 0.9921875]\n",
      "iter: 12300, loss: [0.06745858, 0.9765625]\n",
      "iter: 12350, loss: [0.03885181, 0.98828125]\n",
      "iter: 12400, loss: [0.022504568, 0.98828125]\n",
      "iter: 12450, loss: [0.07598789, 0.98046875]\n",
      "iter: 12500, loss: [0.050867252, 0.984375]\n",
      "iter: 12550, loss: [0.034864742, 0.984375]\n",
      "iter: 12600, loss: [0.04765107, 0.98828125]\n",
      "iter: 12650, loss: [0.037422754, 0.98828125]\n",
      "iter: 12700, loss: [0.03627109, 0.984375]\n",
      "iter: 12750, loss: [0.019274747, 1.0]\n",
      "iter: 12800, loss: [0.052406967, 0.97265625]\n",
      "iter: 12850, loss: [0.04001003, 0.98828125]\n",
      "iter: 12900, loss: [0.062335066, 0.984375]\n",
      "iter: 12950, loss: [0.023886392, 0.9921875]\n",
      "iter: 13000, loss: [0.09923885, 0.96484375]\n",
      "iter: 13050, loss: [0.06270794, 0.9765625]\n",
      "iter: 13100, loss: [0.03072409, 0.98828125]\n",
      "iter: 13150, loss: [0.046017304, 0.98046875]\n",
      "iter: 13200, loss: [0.02421812, 0.984375]\n",
      "iter: 13250, loss: [0.044999532, 0.984375]\n",
      "iter: 13300, loss: [0.077908784, 0.9609375]\n",
      "iter: 13350, loss: [0.03122536, 0.984375]\n",
      "iter: 13400, loss: [0.07747781, 0.984375]\n",
      "iter: 13450, loss: [0.049538806, 0.98046875]\n",
      "iter: 13500, loss: [0.045443717, 0.9765625]\n",
      "iter: 13550, loss: [0.053084522, 0.984375]\n",
      "iter: 13600, loss: [0.034056067, 0.98828125]\n",
      "iter: 13650, loss: [0.048644967, 0.9921875]\n",
      "iter: 13700, loss: [0.05558299, 0.984375]\n",
      "iter: 13750, loss: [0.039809737, 0.984375]\n",
      "iter: 13800, loss: [0.03397782, 0.984375]\n",
      "iter: 13850, loss: [0.05902339, 0.98828125]\n",
      "val loss, [0.06848405539868814, 0.9766257591093117]\n",
      "iter: 13900, loss: [0.036325675, 0.9921875]\n",
      "iter: 13950, loss: [0.028791346, 0.98828125]\n",
      "iter: 14000, loss: [0.076492324, 0.98828125]\n",
      "iter: 14050, loss: [0.029616363, 0.9921875]\n",
      "iter: 14100, loss: [0.100610785, 0.97265625]\n",
      "iter: 14150, loss: [0.025129119, 0.9921875]\n",
      "iter: 14200, loss: [0.02808297, 0.98828125]\n",
      "iter: 14250, loss: [0.046421356, 0.98046875]\n",
      "iter: 14300, loss: [0.038460657, 0.9921875]\n",
      "iter: 14350, loss: [0.037983038, 0.984375]\n",
      "iter: 14400, loss: [0.087801754, 0.9765625]\n",
      "iter: 14450, loss: [0.047138758, 0.9921875]\n",
      "iter: 14500, loss: [0.04903586, 0.98046875]\n",
      "iter: 14550, loss: [0.03598295, 0.9921875]\n",
      "iter: 14600, loss: [0.029416412, 0.98828125]\n",
      "iter: 14650, loss: [0.04222243, 0.98828125]\n",
      "iter: 14700, loss: [0.020866212, 0.99609375]\n",
      "iter: 14750, loss: [0.06456423, 0.98046875]\n",
      "iter: 14800, loss: [0.030016836, 0.98828125]\n",
      "iter: 14850, loss: [0.051122546, 0.98046875]\n",
      "iter: 14900, loss: [0.064070545, 0.98046875]\n",
      "iter: 14950, loss: [0.030709606, 0.98828125]\n",
      "iter: 15000, loss: [0.035754498, 0.984375]\n",
      "iter: 15050, loss: [0.03657546, 0.9921875]\n",
      "iter: 15100, loss: [0.049478117, 0.97265625]\n",
      "iter: 15150, loss: [0.078116395, 0.97265625]\n",
      "iter: 15200, loss: [0.05031558, 0.9765625]\n",
      "iter: 15250, loss: [0.0306215, 0.98828125]\n",
      "iter: 15300, loss: [0.027509032, 0.984375]\n",
      "iter: 15350, loss: [0.065967515, 0.97265625]\n",
      "iter: 15400, loss: [0.066519424, 0.98046875]\n",
      "iter: 15450, loss: [0.04891269, 0.98046875]\n",
      "iter: 15500, loss: [0.031581573, 0.98828125]\n",
      "iter: 15550, loss: [0.078622095, 0.984375]\n",
      "iter: 15600, loss: [0.05778311, 0.9765625]\n",
      "val loss, [0.06888399300304984, 0.9769104251012146]\n",
      "iter: 15650, loss: [0.03350996, 0.98828125]\n",
      "iter: 15700, loss: [0.03762319, 0.98828125]\n",
      "iter: 15750, loss: [0.050994128, 0.9921875]\n",
      "iter: 15800, loss: [0.03995428, 0.98046875]\n",
      "iter: 15850, loss: [0.0646625, 0.98046875]\n",
      "iter: 15900, loss: [0.0350031, 0.98828125]\n",
      "iter: 15950, loss: [0.037007563, 0.98828125]\n",
      "iter: 16000, loss: [0.020284029, 0.99609375]\n",
      "iter: 16050, loss: [0.046771355, 0.98046875]\n",
      "iter: 16100, loss: [0.032649983, 0.98828125]\n",
      "iter: 16150, loss: [0.055386692, 0.984375]\n",
      "iter: 16200, loss: [0.019577604, 0.9921875]\n",
      "iter: 16250, loss: [0.018303059, 0.99609375]\n",
      "iter: 16300, loss: [0.05727429, 0.9765625]\n",
      "iter: 16350, loss: [0.054478046, 0.98828125]\n",
      "iter: 16400, loss: [0.039151855, 0.984375]\n",
      "iter: 16450, loss: [0.028138097, 0.9921875]\n",
      "iter: 16500, loss: [0.047624044, 0.984375]\n",
      "iter: 16550, loss: [0.05545741, 0.98046875]\n",
      "iter: 16600, loss: [0.025391644, 0.9921875]\n",
      "iter: 16650, loss: [0.034781598, 0.99609375]\n",
      "iter: 16700, loss: [0.061275613, 0.9765625]\n",
      "iter: 16750, loss: [0.05637566, 0.984375]\n",
      "iter: 16800, loss: [0.037652504, 0.98828125]\n",
      "iter: 16850, loss: [0.048984874, 0.98046875]\n",
      "iter: 16900, loss: [0.03413604, 0.98828125]\n",
      "iter: 16950, loss: [0.057032965, 0.9765625]\n",
      "iter: 17000, loss: [0.048478127, 0.98046875]\n",
      "iter: 17050, loss: [0.031290777, 0.9921875]\n",
      "iter: 17100, loss: [0.052526176, 0.9765625]\n",
      "iter: 17150, loss: [0.09694188, 0.96875]\n",
      "iter: 17200, loss: [0.04771188, 0.98828125]\n",
      "iter: 17250, loss: [0.07479956, 0.96484375]\n",
      "iter: 17300, loss: [0.022092976, 0.9921875]\n"
     ]
    }
   ],
   "source": [
    "# train classifier\n",
    "iters_per_sample = train_bs_num\n",
    "total_iter = train_bs_num * 10\n",
    "\n",
    "best_val = 100000.0\n",
    "best_result = []\n",
    "\n",
    "for i in range(total_iter):\n",
    "    text, label = next(train_gen)[0]\n",
    "    K.set_value(cls_train.optimizer.lr, 1e-4)\n",
    "    loss = cls_train.train_on_batch(\n",
    "        [text, label], None)\n",
    "  \n",
    "    if i % 50 == 0:\n",
    "        print ('iter: %s, loss: %s' % (i, loss))\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "    if i % train_bs_num == 0 :\n",
    "        val_loss = cls_train.evaluate_generator(val_gen, steps=val_bs_num)\n",
    "        print('val loss,', val_loss)\n",
    "        sys.stdout.flush()\n",
    "        if val_loss[0] <= best_val:\n",
    "            best_val = val_loss[0]\n",
    "            print('saving weights with best val:', val_loss)\n",
    "            sys.stdout.flush()\n",
    "            cls.save_weights('pretrain/yelp/cls.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
