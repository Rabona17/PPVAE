{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# encoding=utf-8\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import *\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import Sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras import regularizers\n",
    "from keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, Callback\n",
    "from keras.initializers import Ones, Zeros, Orthogonal\n",
    "from myclass import TiedEmbeddingsTransposed, LayerNormalization,Attention, Position_Embedding\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "\n",
    "import keras.backend.tensorflow_backend as KTF\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True  \n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "pad_token = 0\n",
    "oov_token = 1\n",
    "start_token = 2\n",
    "end_token = 3\n",
    "\n",
    "# load training data\n",
    "# choose the dataset by your need\n",
    "train_pos_path = 'your conditional data'\n",
    "train_neg_path = 'your negative samples'\n",
    "\n",
    "# To set the specific hyper-parameters under each condition, please refer to our paper\n",
    "max_len = 17\n",
    "dp = 0.2 # dropout rate\n",
    "emb_size = 256\n",
    "gru_dim = 150\n",
    "batch_size = 128 \n",
    "latent_dim = 128 # input size of PluginVAE\n",
    "bottle_dim = 20 # bottleneck vector size of PluginVAE\n",
    "beta = K.variable(5.0) # control the KL term\n",
    "kl_weight = 1.0 # weight of KL loss\n",
    "head_num = 8\n",
    "head_size = [(emb_size+latent_dim) // head_num, (emb_size+2*latent_dim) // head_num, (emb_size+3*latent_dim) // head_num]\n",
    "\n",
    "gama = 0.1 # control the effect of negative sample, please refer to equation 5 of our paper\n",
    "\n",
    "train_pos = []\n",
    "train_neg = []\n",
    "\n",
    "with open(train_pos_path, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        line = line.strip().lower()\n",
    "        text = line.split(' ')\n",
    "        train_pos.append(text)\n",
    "        \n",
    "with open(train_neg_path, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        line = line.strip().lower()\n",
    "        text = line.split(' ')\n",
    "        train_neg.append(text)\n",
    "        \n",
    "        \n",
    "print('the size of train positive corpus is:', len(train_pos))\n",
    "print('the size of train negateive corpus is:', len(train_neg))\n",
    "sys.stdout.flush()\n",
    "\n",
    "# load vocab file\n",
    "if os.path.exists('yelp-vocab.json'):\n",
    "    chars,id2char,char2id = json.load(open('yelp-vocab.json'))\n",
    "    id2char = {int(i):j for i,j in id2char.items()}\n",
    "\n",
    "print('vocab size:', len(char2id))\n",
    "sys.stdout.flush()\n",
    "        \n",
    "\n",
    "def str2id(s, start_end = False):\n",
    "    ids = [char2id.get(c, oov_token) for c in s]\n",
    "    if start_end:\n",
    "        ids = [start_token] + ids + [end_token]\n",
    "  \n",
    "    return ids\n",
    "\n",
    "def padding(x,y,z):\n",
    "    ml = max_len\n",
    "    x = [i + [0] * (ml-len(i)) for i in x]\n",
    "    y = [i + [0] * (ml-len(i)) for i in y]\n",
    "    z = [i + [0] * (ml-len(i)) for i in z]\n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "    z = np.array(z)\n",
    "    \n",
    "    return x,y, z\n",
    "    \n",
    "def train_generator(data, bs):\n",
    "    x = []\n",
    "    y = []\n",
    "    z = []\n",
    "    \n",
    "    while True:\n",
    "        np.random.shuffle(data)    \n",
    "        for d in data:\n",
    "            if len(d) > (max_len-2):\n",
    "                d = d[:max_len-2]\n",
    "                \n",
    "            d = str2id(d, start_end=True)\n",
    "            \n",
    "            x.append(d)\n",
    "            y.append(d)\n",
    "            z.append(d[1:])\n",
    "\n",
    "            if len(x) == bs:\n",
    "                x,y,z = padding(x, y, z)\n",
    "\n",
    "                yield [x,y,z], None\n",
    "                x = []\n",
    "                y = []   \n",
    "                z = []\n",
    "\n",
    "def sample(preds, diversity=1.0):\n",
    "    # sample from the given prediction\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / diversity\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "\n",
    "    return np.argmax(probas)\n",
    "\n",
    "def argmax(preds):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    return np.argmax(preds)\n",
    "\n",
    "def gen(num, diversity, argmax_flag):\n",
    "    print('----- Generating from Generator-----')\n",
    "    start_index = start_token #<BOS>\n",
    "    start_word = id2char[start_index]\n",
    "    for i in range(num):\n",
    "        noise_vec = np.random.normal(size=(1, bottle_dim))\n",
    "        g_vec = decoder.predict(noise_vec)\n",
    "        generated = [[start_index]]\n",
    "        sys.stdout.write(start_word)\n",
    "        sys.stdout.flush()\n",
    "\n",
    "        while(end_token not in generated[0] and len(generated[0]) <= max_len):\n",
    "            x_seq = pad_sequences(generated, maxlen=max_len,padding='post')\n",
    "            preds = dec_model.predict([x_seq, x_seq, g_vec], verbose=0)[0]\n",
    "            preds = preds[len(generated[0])-1][3:]\n",
    "            if argmax_flag:\n",
    "                next_index = argmax(preds)\n",
    "            else:\n",
    "                next_index = sample(preds, diversity)\n",
    "\n",
    "            next_index += 3\n",
    "            next_word = id2char[next_index]\n",
    "\n",
    "            generated[0] += [next_index]\n",
    "            sys.stdout.write(next_word+' ')\n",
    "            sys.stdout.flush()\n",
    "        print('\\n')\n",
    "        \n",
    "        \n",
    "def gen_random(num, diversity, argmax_flag):\n",
    "    print('----- Generating from random-----')\n",
    "    start_index = start_token #<BOS>\n",
    "    start_word = id2char[start_index]\n",
    "    for i in range(num):\n",
    "        noise_vec = np.random.normal(size=(1, latent_dim))\n",
    "        generated = [[start_index]]\n",
    "        sys.stdout.write(start_word)\n",
    "        sys.stdout.flush()\n",
    "\n",
    "        while(end_token not in generated[0] and len(generated[0]) <= max_len):\n",
    "            x_seq = pad_sequences(generated, maxlen=max_len,padding='post')\n",
    "            preds = dec_model.predict([x_seq, x_seq, noise_vec], verbose=0)[0]\n",
    "            preds = preds[len(generated[0])-1][3:]\n",
    "            if argmax_flag:\n",
    "                next_index = argmax(preds)\n",
    "            else:\n",
    "                next_index = sample(preds, diversity)\n",
    "\n",
    "            next_index += 3\n",
    "            next_word = id2char[next_index]\n",
    "\n",
    "            generated[0] += [next_index]\n",
    "            sys.stdout.write(next_word+' ')\n",
    "            sys.stdout.flush()\n",
    "        print('\\n')\n",
    "\n",
    "\n",
    "train_pos_gen = train_generator(train_pos, batch_size)\n",
    "train_neg_gen = train_generator(train_neg, batch_size)\n",
    "\n",
    "# model archietecture of PretrainVAE\n",
    "\n",
    "#encoder\n",
    "encoder_input = Input(shape=(max_len, ), dtype='int32')\n",
    "emb_layer = Embedding(len(char2id), emb_size)\n",
    "encoder_emb = emb_layer(encoder_input) \n",
    "encoder1 = Bidirectional(GRU(gru_dim))\n",
    "encoder_h = encoder1(encoder_emb)\n",
    "\n",
    "z_mean = Dense(latent_dim)(encoder_h)\n",
    "z_log_var = Dense(latent_dim)(encoder_h)\n",
    "\n",
    "kl_loss = Lambda(lambda x: K.mean(- 0.5 * K.sum(1 + x[0] - K.square(x[1]) - K.exp(x[0]), axis=-1)))([z_log_var, z_mean])\n",
    "\n",
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    epsilon = K.random_normal(shape=(K.shape(z_mean)[0], latent_dim), mean=0, stddev=1)\n",
    "    return z_mean + K.exp(z_log_var / 2) * epsilon\n",
    "\n",
    "enc_z = Lambda(sampling)([z_mean, z_log_var])\n",
    "enc_z = Lambda(lambda x: K.in_train_phase(x[0], x[1]))([enc_z, z_mean])\n",
    "\n",
    "enc_model = Model(encoder_input, [enc_z, kl_loss])\n",
    "enc_model.load_weights('pretrain/yelp/enc-base-another.h5')\n",
    "print('load encoder weights successfully')\n",
    "\n",
    "\n",
    "# decoder\n",
    "decoder_input = Input(shape=(max_len,), dtype='int32')\n",
    "decoder_z_input = Input(shape=(latent_dim, ))\n",
    "decoder_true_output = Input(shape=(max_len,), dtype='int32')\n",
    "\n",
    "\n",
    "decoder_dense = Dense(emb_size)\n",
    "dec_softmax = TiedEmbeddingsTransposed(tied_to=emb_layer, activation='softmax')\n",
    "\n",
    "decoder_emb = emb_layer(decoder_input)\n",
    "decoder_emb = Position_Embedding()(decoder_emb)\n",
    "decoder_z = RepeatVector(max_len)(decoder_z_input)\n",
    "decoder_h = decoder_emb\n",
    "\n",
    "for layer in range(3):\n",
    "    decoder_z_hier = Dense(latent_dim, activation=None)(decoder_z)\n",
    "    decoder_h = Concatenate()([decoder_h, decoder_z_hier])\n",
    "    decoder_h_attn = Attention(head_num, head_size[layer], max_len)([decoder_h, decoder_h, decoder_h])\n",
    "    decoder_h = Add()([decoder_h, decoder_h_attn])\n",
    "    decoder_h = LayerNormalization()(decoder_h)\n",
    "    decoder_h_mlp = Dense(head_size[layer]*head_num, activation='relu')(decoder_h)\n",
    "    decoder_h = Add()([decoder_h, decoder_h_mlp])\n",
    "    decoder_h = LayerNormalization()(decoder_h)\n",
    "    decoder_h = Position_Embedding()(decoder_h)\n",
    "\n",
    "\n",
    "decoder_h = decoder_dense(decoder_h)\n",
    "decoder_output = dec_softmax(decoder_h)\n",
    "\n",
    "dec_model = Model([decoder_input, decoder_true_output, decoder_z_input], decoder_output)\n",
    "dec_model.load_weights('pretrain/yelp/dec-base-another.h5')\n",
    "print('load decoder weights successfully')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model architecture of PluginVAE\n",
    "\n",
    "#encoder\n",
    "z_in = Input(shape=(latent_dim, ))\n",
    "z = z_in\n",
    "z = Dense(latent_dim//2, activation=None)(z)\n",
    "z = LeakyReLU()(z)\n",
    "z = Dense(latent_dim//4, activation=None)(z)\n",
    "z = LeakyReLU()(z)\n",
    "\n",
    "z_mean = Dense(bottle_dim)(z)\n",
    "z_log_var = Dense(bottle_dim)(z)\n",
    "kl_loss = Lambda(lambda x: K.mean(- 0.5 * K.sum(1 + x[0] - K.square(x[1]) - K.exp(x[0]), axis=-1)))([z_log_var, z_mean])\n",
    "\n",
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    epsilon = K.random_normal(shape=(K.shape(z_mean)[0], bottle_dim), mean=0, stddev=1)\n",
    "    return z_mean + K.exp(z_log_var / 2) * epsilon\n",
    "\n",
    "enc_z = Lambda(sampling)([z_mean, z_log_var])\n",
    "enc_z = Lambda(lambda x: K.in_train_phase(x[0], x[1]))([enc_z, z_mean])\n",
    "encoder = Model(z_in, [enc_z, kl_loss])\n",
    "encoder.summary()\n",
    "\n",
    "#decoder\n",
    "dec1 = Dense(latent_dim//4, activation=None)\n",
    "dec2 = Dense(latent_dim//2, activation=None)\n",
    "dec3 = Dense(latent_dim, activation=None)\n",
    "\n",
    "z = dec1(enc_z)\n",
    "z = LeakyReLU()(z)\n",
    "z = dec2(z)\n",
    "z = LeakyReLU()(z)\n",
    "z = dec3(z)\n",
    "\n",
    "z_bottle = Input(shape=(bottle_dim, ))\n",
    "z = dec1(z_bottle)\n",
    "z = LeakyReLU()(z)\n",
    "z = dec2(z)\n",
    "z = LeakyReLU()(z)\n",
    "z = dec3(z)\n",
    "decoder = Model(z_bottle, z)\n",
    "decoder.summary()\n",
    "\n",
    "# define a Model object to do the training job\n",
    "z_in = Input(shape=(latent_dim, ))\n",
    "z_another = Input(shape=(latent_dim, ))\n",
    "\n",
    "rec_z, kl_loss1 = encoder(z_in)\n",
    "rec_z = decoder(rec_z)\n",
    "rec_z_another, kl_loss2 = encoder(z_another)\n",
    "rec_z_another = decoder(rec_z_another)\n",
    "\n",
    "vae_train = Model([z_in, z_another], rec_z)\n",
    "\n",
    "mse_loss1 = K.mean(K.square(rec_z-z_in))\n",
    "mse_loss2 = K.mean(K.square(z_another-rec_z_another))\n",
    "\n",
    "vae_train.add_loss(mse_loss1 + kl_weight*K.abs(kl_loss1-beta) \n",
    "                   + gama*kl_weight*K.abs(kl_loss2-beta)- gama*mse_loss2)\n",
    "vae_train.compile(optimizer=Adam(1e-3, 0.5))\n",
    "vae_train.metrics_names.append('kl_loss1')\n",
    "vae_train.metrics_tensors.append(kl_loss1)\n",
    "vae_train.metrics_names.append('kl_loss2')\n",
    "vae_train.metrics_tensors.append(kl_loss2)\n",
    "vae_train.metrics_names.append('mse_loss1')\n",
    "vae_train.metrics_tensors.append(mse_loss1)\n",
    "vae_train.metrics_names.append('mse_loss2')\n",
    "vae_train.metrics_tensors.append(mse_loss2)\n",
    "vae_train.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load pretrained language model to conduct automatic evaluation\n",
    "lm_emb_size = 128\n",
    "lm_gru_dim = 1024\n",
    "lm_max_len = 17\n",
    "lm_input = Input(shape=(lm_max_len, ), dtype='int32')\n",
    "lm_true_output = Input(shape=(lm_max_len,), dtype='int32')\n",
    "lm_true_mask = Lambda(lambda x: K.cast(K.greater(K.expand_dims(x, 2), 0), 'float32'))(lm_true_output)\n",
    "emb_layer = Embedding(len(char2id), lm_emb_size)\n",
    "lm_emb = emb_layer(lm_input) # id转向量\n",
    "\n",
    "gru = GRU(lm_gru_dim, return_sequences=True)\n",
    "lm_dense = Dense(lm_emb_size)\n",
    "lm_softmax = TiedEmbeddingsTransposed(tied_to=emb_layer, activation='softmax')\n",
    "\n",
    "lm_emb = SpatialDropout1D(dp)(lm_emb)\n",
    "lm_h = gru(lm_emb)\n",
    "lm_h = lm_dense(lm_h)\n",
    "lm_output = lm_softmax(lm_h)\n",
    "\n",
    "xent_loss = K.sum(K.sparse_categorical_crossentropy(lm_true_output, lm_output)*lm_true_mask[:,:,0])/K.sum(lm_true_mask[:,:,0])\n",
    "lm_model = Model([lm_input, lm_true_output], lm_output)\n",
    "lm_model.load_weights('pretrain/yelp/LM.h5')\n",
    "lm_model.add_loss(xent_loss)\n",
    "lm_model.compile(Adam(1e-3))\n",
    "print('load language model successfully')\n",
    "lm_model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#load pretrained classifier to conduct automatic evaluation\n",
    "# notice length-condition doesn't need it \n",
    "cnn_filter = 400\n",
    "cnn_kernel = 3\n",
    "x_in = Input(shape=(max_len, ))\n",
    "x = x_in\n",
    "x = Embedding(len(char2id), 100)(x)\n",
    "x = Conv1D(cnn_filter, cnn_kernel, padding='valid', activation='relu')(x)\n",
    "x = GlobalMaxPooling1D()(x)\n",
    "x_out = Dense(1, activation='sigmoid')(x)\n",
    "cls = Model(x_in ,x_out)\n",
    "#cls.summary()\n",
    "cls.load_weights('pretrain/yelp/cls.h5')\n",
    "print('load clssifier successfully')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_from_ae(diversity, num, argmax_flag=False):\n",
    "    r1 = []\n",
    "    r2 = []\n",
    "    r3 = []\n",
    "    start_index = start_token #<BOS>\n",
    "    start_word = id2char[start_index]\n",
    "    for j in range(num):\n",
    "        random_vec = np.random.normal(size=(1, bottle_dim))\n",
    "        g_vec = decoder.predict(random_vec)\n",
    "        generated = [[start_index]]\n",
    "        gen_word = []\n",
    "        while(end_token not in generated[0] and len(generated[0]) <= max_len):\n",
    "            x_seq = pad_sequences(generated, maxlen=max_len,padding='post')\n",
    "            preds = dec_model.predict([x_seq, x_seq, g_vec], verbose=0)[0]\n",
    "            preds = preds[len(generated[0])-1][3:]\n",
    "            if argmax_flag:\n",
    "                next_index = argmax(preds)\n",
    "            else:\n",
    "                next_index = sample(preds, diversity)\n",
    "\n",
    "            next_index += 3\n",
    "            next_word = id2char[next_index]\n",
    "            gen_word.append(next_word)\n",
    "            generated[0] += [next_index]\n",
    "\n",
    "        if '<EOS>' == gen_word[-1]:\n",
    "            gen_word = gen_word[:-1]\n",
    "        gen_word = gen_word[:(max_len-2)]    \n",
    "        r1.append([char2id[c] for c in gen_word])\n",
    "        r2.append([start_token]+[char2id[c] for c in gen_word]+[end_token])\n",
    "        r3.append([char2id[c] for c in gen_word]+[end_token])\n",
    "    return np.array(r1), np.array(r2), np.array(r3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distinct(id_list_data):\n",
    "    # get distinct-1/2\n",
    "    grams = id_list_data\n",
    "    grams_list1 = []\n",
    "    for sen in grams:\n",
    "        for g in sen:\n",
    "            grams_list1.append(g)\n",
    "            \n",
    "    grams_list2 = []\n",
    "    for sen in grams:\n",
    "        for i in range(len(sen)-1):\n",
    "            grams_list2.append(str(sen[i])+' '+str(sen[i+1]))\n",
    "            \n",
    "    print('distinct-1:', len(set(grams_list1))/len(grams_list1))\n",
    "    print('distinct-2:', len(set(grams_list2))/len(grams_list2))\n",
    "    \n",
    "def gen_from_vec(diversity, vec, argmax_flag):\n",
    "    start_index = start_token #<BOS>\n",
    "    start_word = id2char[start_index]\n",
    "    print()\n",
    "\n",
    "    generated = [[start_index]]\n",
    "    sys.stdout.write(start_word)\n",
    "\n",
    "    while(end_token not in generated[0] and len(generated[0]) <= max_len):\n",
    "        x_seq = pad_sequences(generated, maxlen=max_len,padding='post')\n",
    "        preds = dec_model.predict([x_seq, x_seq, vec], verbose=0)[0]\n",
    "        preds = preds[len(generated[0])-1][3:]\n",
    "        if argmax_flag:\n",
    "            next_index = argmax(preds)\n",
    "        else:\n",
    "            next_index = sample(preds, diversity)\n",
    "        next_index += 3\n",
    "        next_word = id2char[next_index]\n",
    "\n",
    "        generated[0] += [next_index]\n",
    "        sys.stdout.write(next_word+' ')\n",
    "        sys.stdout.flush()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# training process\n",
    "total_iter = 20001\n",
    "\n",
    "best_val = 100000.0\n",
    "best_result = []\n",
    "\n",
    "# to set weight beta, please refer to our paper\n",
    "def get_beta_weight(iter_num):\n",
    "    now_beta_weight = min((5.0/10000)*iter_num, 5.0)\n",
    "    return now_beta_weight\n",
    "\n",
    "for i in range(total_iter):\n",
    "    real_x1 = next(train_pos_gen)[0][0]\n",
    "    real_z1, _ = enc_model.predict(real_x1)\n",
    "    real_x2 = next(train_neg_gen)[0][0]\n",
    "    real_z2, _ = enc_model.predict(real_x2)\n",
    "    \n",
    "    K.set_value(vae_train.optimizer.lr, 3e-4)\n",
    "    K.set_value(beta, get_beta_weight(i))\n",
    "    loss = vae_train.train_on_batch(\n",
    "        [real_z1, real_z2], None)\n",
    "        \n",
    "    if i % 50 == 0:\n",
    "        print ('iter: %s, loss: %s' % (i, loss))\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "    if (i % 2000 == 0) and i!=0:\n",
    "        gen_num = 500\n",
    "        gen_samples1, gen_samples2, gen_samples3 = gen_from_ae(1.0, gen_num, True)\n",
    "        get_distinct(gen_samples1)\n",
    "        gen_samples1, gen_samples2, gen_samples3 = padding(gen_samples1, gen_samples2, gen_samples3)\n",
    "        gen_result = cls.predict(gen_samples1)\n",
    "        print('%f of the sample is positive in generator'%(np.sum(np.round(gen_result))/gen\n",
    "        gen(10, 0.5, True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating 10K text for evaluation\n",
    "gen_num = 10000\n",
    "gen_1, gen_2, gen_3 = gen_from_ae(1.0, gen_num, True)\n",
    "with open('gen/PPVAE.txt', 'w', encoding='utf-8') as f:\n",
    "    for g in gen_1:\n",
    "        f.write(' '.join([id2char[index] for index in g])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate distinct-1/2 and condition accurarcy by pre-trained classifier\n",
    "# notice length condition doesn't need classifier\n",
    "get_distinct(gen_1)\n",
    "gen_1, gen_2, gen_3 = padding(gen_1, gen_2, gen_3)\n",
    "cls_result = cls.predict(gen_1)\n",
    "print('%f of the sample is positive'%(np.sum(np.round(cls_result))/gen_num))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}