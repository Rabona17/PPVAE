{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# encoding=utf-8\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import *\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import Sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras import regularizers\n",
    "from keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, Callback\n",
    "from keras.initializers import Ones, Zeros, Orthogonal\n",
    "from myclass import TiedEmbeddingsTransposed, Attention, Position_Embedding, LayerNormalization\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "\n",
    "import keras.backend.tensorflow_backend as KTF\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "\n",
    "pad_token = 0\n",
    "oov_token = 1\n",
    "start_token = 2\n",
    "end_token = 3\n",
    "\n",
    "# choose the dataset by your need\n",
    "train_path = 'your conditional data'\n",
    "\n",
    "# To set the specific hyper-parameters under each condition, please refer to our paper\n",
    "max_len = 17\n",
    "dp = 0.2 # dropout rate\n",
    "emb_size = 256\n",
    "gru_dim = 150\n",
    "batch_size = 128\n",
    "latent_dim = 128 # input size of PluginVAE\n",
    "bottle_dim = 20 # bottleneck vector size of PluginVAE\n",
    "beta = K.variable(5.0) # control the KL term\n",
    "kl_weight = 1.0 # weight of KL loss\n",
    "head_num = 8\n",
    "head_size = [(emb_size+latent_dim) // head_num, (emb_size+2*latent_dim) // head_num, (emb_size+3*latent_dim) // head_num]\n",
    "\n",
    "train = []\n",
    "\n",
    "with open(train_path, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        line = line.strip().lower().split(' ') #已经分好词了\n",
    "        train.append(line)\n",
    "\n",
    "print('train corpus size:', sum([len(d) for d in train]))\n",
    "sys.stdout.flush()\n",
    "print('sequences:', len(train))\n",
    "sys.stdout.flush()\n",
    "\n",
    "if os.path.exists('yelp-vocab.json'):\n",
    "    chars,id2char,char2id = json.load(open('yelp-vocab.json'))\n",
    "    id2char = {int(i):j for i,j in id2char.items()}\n",
    "\n",
    "print('vocab size:', len(char2id))\n",
    "sys.stdout.flush()\n",
    "\n",
    "        \n",
    "print('%d texts in the training set'%len(train))\n",
    "\n",
    "def str2id(s, start_end = False):\n",
    "    ids = [char2id.get(c, oov_token) for c in s]\n",
    "    if start_end:\n",
    "        ids = [start_token] + ids + [end_token]\n",
    "  \n",
    "    return ids\n",
    "\n",
    "def padding(x,y,z):\n",
    "    ml = max_len\n",
    "    x = [i + [0] * (ml-len(i)) for i in x]\n",
    "    y = [i + [0] * (ml-len(i)) for i in y]\n",
    "    z = [i + [0] * (ml-len(i)) for i in z]\n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "    z = np.array(z)\n",
    "    \n",
    "    return x,y, z\n",
    "    \n",
    "def train_generator(yelp_data):\n",
    "    x = []\n",
    "    while True:\n",
    "        np.random.shuffle(yelp_data)    \n",
    "        for yelp in yelp_data:\n",
    "            if len(yelp) > (max_len-2):\n",
    "                yelp = yelp[:max_len-2]\n",
    "                \n",
    "            yelp = str2id(yelp, start_end=True)\n",
    "            x.append(yelp)\n",
    "            if len(x) == batch_size:\n",
    "                x = [i + [0] * (max_len-len(i)) for i in x]\n",
    "                x = np.array(x)\n",
    "                z,_ = enc_model.predict(x)\n",
    "\n",
    "                yield [x,z], None\n",
    "                x = []\n",
    "                z = []\n",
    "\n",
    "def sample(preds, diversity=1.0):\n",
    "    # sample from the given prediction\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / diversity\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "\n",
    "    return np.argmax(probas)\n",
    "\n",
    "def argmax(preds):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    return np.argmax(preds)\n",
    "\n",
    "def gen(num, diversity, argmax_flag):\n",
    "    print('----- Generating from Generator-----')\n",
    "    start_index = start_token #<BOS>\n",
    "    start_word = id2char[start_index]\n",
    "    for i in range(num):\n",
    "        noise_vec = np.random.normal(size=(1, bottle_dim))\n",
    "        g_vec = decoder.predict(noise_vec)\n",
    "        generated = [[start_index]]\n",
    "        sys.stdout.write(start_word)\n",
    "        sys.stdout.flush()\n",
    "\n",
    "        while(end_token not in generated[0] and len(generated[0]) <= max_len):\n",
    "            x_seq = pad_sequences(generated, maxlen=max_len,padding='post')\n",
    "            preds = dec_model.predict([x_seq, x_seq, g_vec], verbose=0)[0]\n",
    "            preds = preds[len(generated[0])-1][3:]\n",
    "            if argmax_flag:\n",
    "                next_index = argmax(preds)\n",
    "            else:\n",
    "                next_index = sample(preds, diversity)\n",
    "\n",
    "            next_index += 3\n",
    "            next_word = id2char[next_index]\n",
    "\n",
    "            generated[0] += [next_index]\n",
    "            sys.stdout.write(next_word+' ')\n",
    "            sys.stdout.flush()\n",
    "        print('\\n')\n",
    "        \n",
    "        \n",
    "train_gen = train_generator(train)\n",
    "\n",
    "#model architecture of PretrainVAE \n",
    "\n",
    "#encoder\n",
    "encoder_input = Input(shape=(max_len, ), dtype='int32')\n",
    "emb_layer = Embedding(len(char2id), emb_size)\n",
    "encoder_emb = emb_layer(encoder_input) \n",
    "encoder1 = Bidirectional(GRU(gru_dim))\n",
    "\n",
    "encoder_h = encoder1(encoder_emb)\n",
    "\n",
    "# re-parameteristic trick\n",
    "z_mean = Dense(latent_dim)(encoder_h)\n",
    "z_log_var = Dense(latent_dim)(encoder_h)\n",
    "\n",
    "kl_loss = Lambda(lambda x: K.mean(- 0.5 * K.sum(1 + x[0] - K.square(x[1]) - K.exp(x[0]), axis=-1)))([z_log_var, z_mean])\n",
    "\n",
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    epsilon = K.random_normal(shape=(K.shape(z_mean)[0], latent_dim), mean=0, stddev=1)\n",
    "    return z_mean + K.exp(z_log_var / 2) * epsilon\n",
    "\n",
    "enc_z = Lambda(sampling)([z_mean, z_log_var])\n",
    "enc_z = Lambda(lambda x: K.in_train_phase(x[0], x[1]))([enc_z, z_mean])\n",
    "\n",
    "enc_model = Model(encoder_input, [enc_z, kl_loss])\n",
    "enc_model.load_weights('pretrain/yelp/enc-base-another.h5')\n",
    "print('load encoder weights successfully')\n",
    "\n",
    "# decoder\n",
    "decoder_input = Input(shape=(max_len,), dtype='int32')\n",
    "decoder_z_input = Input(shape=(latent_dim, ))\n",
    "decoder_true_output = Input(shape=(max_len,), dtype='int32')\n",
    "\n",
    "\n",
    "decoder_dense = Dense(emb_size)\n",
    "dec_softmax = TiedEmbeddingsTransposed(tied_to=emb_layer, activation='softmax')\n",
    "\n",
    "decoder_emb = emb_layer(decoder_input)\n",
    "decoder_emb = Position_Embedding()(decoder_emb)\n",
    "decoder_z = RepeatVector(max_len)(decoder_z_input)\n",
    "decoder_h = decoder_emb\n",
    "\n",
    "for layer in range(3):\n",
    "    decoder_z_hier = Dense(latent_dim, activation=None)(decoder_z)\n",
    "    decoder_h = Concatenate()([decoder_h, decoder_z_hier])\n",
    "    decoder_h_attn = Attention(head_num, head_size[layer], max_len)([decoder_h, decoder_h, decoder_h])\n",
    "    decoder_h = Add()([decoder_h, decoder_h_attn])\n",
    "    decoder_h = LayerNormalization()(decoder_h)\n",
    "    decoder_h_mlp = Dense(head_size[layer]*head_num, activation='relu')(decoder_h)\n",
    "    decoder_h = Add()([decoder_h, decoder_h_mlp])\n",
    "    decoder_h = LayerNormalization()(decoder_h)\n",
    "    decoder_h = Position_Embedding()(decoder_h)\n",
    "\n",
    "\n",
    "decoder_h = decoder_dense(decoder_h)\n",
    "decoder_output = dec_softmax(decoder_h)\n",
    "\n",
    "dec_model = Model([decoder_input, decoder_true_output, decoder_z_input], decoder_output)\n",
    "dec_model.load_weights('pretrain/yelp/dec-base-another.h5')\n",
    "print('load PretrainVAE decoder weights successfully')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model architecture of PluginVAE\n",
    "\n",
    "#encoder\n",
    "z_in = Input(shape=(latent_dim, ))\n",
    "z = z_in\n",
    "z = Dense(latent_dim//2, activation=None)(z)\n",
    "z = LeakyReLU()(z)\n",
    "z = Dense(latent_dim//4, activation=None)(z)\n",
    "z = LeakyReLU()(z)\n",
    "\n",
    "z_mean = Dense(bottle_dim)(z)\n",
    "z_log_var = Dense(bottle_dim)(z)\n",
    "kl_loss = Lambda(lambda x: K.mean(- 0.5 * K.sum(1 + x[0] - K.square(x[1]) - K.exp(x[0]), axis=-1)))([z_log_var, z_mean])\n",
    "\n",
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    epsilon = K.random_normal(shape=(K.shape(z_mean)[0], bottle_dim), mean=0, stddev=1)\n",
    "    return z_mean + K.exp(z_log_var / 2) * epsilon\n",
    "\n",
    "enc_z = Lambda(sampling)([z_mean, z_log_var])\n",
    "enc_z = Lambda(lambda x: K.in_train_phase(x[0], x[1]))([enc_z, z_mean])\n",
    "encoder = Model(z_in, enc_z)\n",
    "encoder.summary()\n",
    "\n",
    "#decoder\n",
    "dec1 = Dense(latent_dim//4, activation=None)\n",
    "dec2 = Dense(latent_dim//2, activation=None)\n",
    "dec3 = Dense(latent_dim, activation=None)\n",
    "\n",
    "z = dec1(enc_z)\n",
    "z = LeakyReLU()(z)\n",
    "z = dec2(z)\n",
    "z = LeakyReLU()(z)\n",
    "z = dec3(z)\n",
    "vae = Model(z_in, z)\n",
    "mse_loss = K.mean(K.square(z-z_in))\n",
    "vae.add_loss(mse_loss + kl_weight*K.abs(kl_loss-beta))\n",
    "vae.compile(optimizer=Adam(1e-3, 0.5))\n",
    "vae.metrics_names.append('kl_loss')\n",
    "vae.metrics_tensors.append(kl_loss)\n",
    "vae.summary()\n",
    "\n",
    "z_bottle = Input(shape=(bottle_dim, ))\n",
    "z = dec1(z_bottle)\n",
    "z = LeakyReLU()(z)\n",
    "z = dec2(z)\n",
    "z = LeakyReLU()(z)\n",
    "z = dec3(z)\n",
    "decoder = Model(z_bottle, z)\n",
    "decoder.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#load pretrained classifier to conduct automatic evaluation\n",
    "# notice length condition doesn't need classifier\n",
    "cnn_filter = 400\n",
    "cnn_kernel = 3\n",
    "x_in = Input(shape=(max_len, ))\n",
    "x = x_in\n",
    "x = Embedding(len(char2id), 100)(x)\n",
    "x = Conv1D(cnn_filter, cnn_kernel, padding='valid', activation='relu')(x)\n",
    "x = GlobalMaxPooling1D()(x)\n",
    "x_out = Dense(1, activation='sigmoid')(x)\n",
    "cls = Model(x_in ,x_out)\n",
    "#cls.summary()\n",
    "cls.load_weights('pretrain/yelp/cls.h5')\n",
    "print('load clssifier successfully')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_from_ae(diversity, num, argmax_flag=False):\n",
    "    r1 = []\n",
    "    r2 = []\n",
    "    r3 = []\n",
    "    start_index = start_token #<BOS>\n",
    "    start_word = id2char[start_index]\n",
    "    for j in range(num):\n",
    "        random_vec = np.random.normal(size=(1, bottle_dim))\n",
    "        g_vec = decoder.predict(random_vec)\n",
    "        generated = [[start_index]]\n",
    "        gen_word = []\n",
    "        while(end_token not in generated[0] and len(generated[0]) <= max_len):\n",
    "            x_seq = pad_sequences(generated, maxlen=max_len,padding='post')\n",
    "            preds = dec_model.predict([x_seq, x_seq, g_vec], verbose=0)[0]\n",
    "            preds = preds[len(generated[0])-1][3:]\n",
    "            if argmax_flag:\n",
    "                next_index = argmax(preds)\n",
    "            else:\n",
    "                next_index = sample(preds, diversity)\n",
    "\n",
    "            next_index += 3\n",
    "            next_word = id2char[next_index]\n",
    "            gen_word.append(next_word)\n",
    "            generated[0] += [next_index]\n",
    "\n",
    "        if '<EOS>' == gen_word[-1]:\n",
    "            gen_word = gen_word[:-1]\n",
    "        gen_word = gen_word[:(max_len-2)]    \n",
    "        r1.append([char2id[c] for c in gen_word])\n",
    "        r2.append([start_token]+[char2id[c] for c in gen_word]+[end_token])\n",
    "        r3.append([char2id[c] for c in gen_word]+[end_token])\n",
    "    return np.array(r1), np.array(r2), np.array(r3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distinct(id_list_data):\n",
    "    grams = id_list_data\n",
    "    grams_list1 = []\n",
    "    for sen in grams:\n",
    "        for g in sen:\n",
    "            grams_list1.append(g)\n",
    "            \n",
    "    grams_list2 = []\n",
    "    for sen in grams:\n",
    "        for i in range(len(sen)-1):\n",
    "            grams_list2.append(str(sen[i])+' '+str(sen[i+1]))\n",
    "            \n",
    "    print('distinct-1:', len(set(grams_list1))/len(grams_list1))\n",
    "    print('distinct-2:', len(set(grams_list2))/len(grams_list2))\n",
    "      \n",
    "def gen_from_vec(diversity, vec, argmax_flag):\n",
    "    start_index = start_token #<BOS>\n",
    "    start_word = id2char[start_index]\n",
    "    print()\n",
    "\n",
    "    generated = [[start_index]]\n",
    "    sys.stdout.write(start_word)\n",
    "\n",
    "    while(end_token not in generated[0] and len(generated[0]) <= max_len):\n",
    "        x_seq = pad_sequences(generated, maxlen=max_len,padding='post')\n",
    "        preds = dec_model.predict([x_seq, x_seq, vec], verbose=0)[0]\n",
    "        preds = preds[len(generated[0])-1][3:]\n",
    "        if argmax_flag:\n",
    "            next_index = argmax(preds)\n",
    "        else:\n",
    "            next_index = sample(preds, diversity)\n",
    "        next_index += 3\n",
    "        next_word = id2char[next_index]\n",
    "\n",
    "        generated[0] += [next_index]\n",
    "        sys.stdout.write(next_word+' ')\n",
    "        sys.stdout.flush()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# training process\n",
    "total_iter = 20001\n",
    "\n",
    "best_val = 100000.0\n",
    "best_result = []\n",
    "\n",
    "# to set weight beta, please refer to our paper\n",
    "def get_beta_weight(iter_num):\n",
    "    now_beta_weight = min((5.0/10000)*iter_num, 5.0)\n",
    "    return now_beta_weight\n",
    "\n",
    "for i in range(total_iter):\n",
    "    real_x, real_z = next(train_gen)[0]\n",
    "    K.set_value(vae.optimizer.lr, 3e-4)\n",
    "    K.set_value(beta, get_beta_weight(i))\n",
    "    loss = vae.train_on_batch(\n",
    "        real_z, None)\n",
    "        \n",
    "    if i % 100 == 0:\n",
    "        print ('iter: %s, loss: %s' % (i, loss))\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "    if (i % 2000 == 0) and i!=0:        \n",
    "        gen_num = 1000\n",
    "        gen_samples1, gen_samples2, gen_samples3 = gen_from_ae(1.0, gen_num, True)\n",
    "        get_distinct(gen_samples1)\n",
    "        gen_samples1, gen_samples2, gen_samples3 = padding(gen_samples1, gen_samples2, gen_samples3)\n",
    "        gen_result = cls.predict(gen_samples1)\n",
    "        print('%f of the sample is positive in generator'%(np.sum(np.round(gen_result))/gen_num))\n",
    "        gen(10, 0.5, True)\n",
    "    \n",
    "        \n",
    "\n",
    "\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating 10K text for evaluation\n",
    "gen_num = 10000\n",
    "gen_1, gen_2, gen_3 = gen_from_ae(1.0, gen_num, True)\n",
    "with open('gen/PPVAE-single.txt', 'w', encoding='utf-8') as f:\n",
    "    for g in gen_1:\n",
    "        f.write(' '.join([id2char[index] for index in g])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get distinct-1/2\n",
    "get_distinct(gen_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# condition accurarcy by pre-trained classifier\n",
    "# notice length condition doesn't need classifier\n",
    "gen_1, gen_2, gen_3 = padding(gen_1, gen_2, gen_3)\n",
    "cls_result = cls.predict(gen_1)\n",
    "print('%f of the sample is positive'%(np.sum(np.round(cls_result))/gen_num))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}